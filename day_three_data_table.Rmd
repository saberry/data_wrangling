---
title: "Using data.table -- R's third dialect"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# What Is It?

The most simple descriptor is that data.table extends base R's functionality and makes it faster. It also borrows from dplyr's notion of piping operations. A departure from typical base R is that data.table includes an extra position in the brackets.

We've already seen fread, so let's continue to use it:

```{r}
library(data.table)

nba_data <- fread("https://www.nd.edu/~sberry5/data/nba_data.csv")

str(nba_data)
```

## Selection

Very similar to base R, but with a very convenient difference!

```{r}
nba_data[, list(PTS, TEAM_ID, GAME_ID)]

nba_data[, list(PTS, pt_calc = PTS*1.5, TEAM_ID, GAME_ID)]
```

## Filtering

Again, very base like.

```{r}
nba_data[PTS > 10, ]

nba_data[PTS > 10]
```

## Grouping

The group_by and summarize from dplyr can be handled with one line:

```{r}
nba_data[, list(avg_pts = mean(PTS)), by = list(TEAM_ID, GAME_ID)]
```

## Special Symbols

```{r}
nba_data[, .N]

nba_data[, .N, by = PLAYER_NAME]
```

```{r}
nba_data[, grp := .GRP, by = list(PLAYER_NAME)]
```

```{r}
setkey(nba_data, PLAYER_NAME)
```

```{r}
nba_data[, player_game_number := sequence(.N), by = key(nba_data)]
```

```{r}
nba_data[, .SD, .SDcols = FGM:FT_PCT]

nba_data[, .SD, .SDcols = c("PLAYER_NAME", "FGM")]

nba_data[, .SD, .SDcols = patterns("PCT")]
```

```{r}
fg_vars <- c("FG_PCT", "FG3_PCT", "FT_PCT")

nba_data[,  lapply(.SD, function(x) x * 100), .SDcols = fg_vars]
```

## Chaining Indices

```{r}
top_pt_performance <- 
  nba_data[, 
           list(player_game_number = sequence(.N), PTS), 
           by = key(nba_data)
  ][
    order(PTS), 
    .SD[.N], 
    by = PLAYER_NAME
  ][PTS > 40]
```

## Merging

What comes below might not be immediately clear, but stick with me for a minute:

```{r}
library(glue)
library(httr)
library(jsonlite)

all_players <- purrr::map_df(0:38, ~{
  Sys.sleep(runif(1, 0, 1))
  
  base_link <- glue("https://www.balldontlie.io/api/v1/players?page={.x}&per_page=100")
  
  request <- GET(base_link)
  
  json_data <- fromJSON(content(request, as = "text"))
  
  player_data <- json_data$data
  
  player_data
})

all_players$full_name <- paste(all_players$first_name, 
                               all_players$last_name, 
                               sep = " ")

all_players <- all_players[, 
                           c("full_name", "height_feet", 
                             "height_inches", "weight_pounds")]

all_players <- as.data.table(all_players)
```

Now that we have that data, we can perform a join!

```{r}
all_players <- all_players[top_pt_performance, on = "full_name==PLAYER_NAME"]
```

```{r}
all_players[, 
            total_height := height_feet * 12 + height_inches
]
```

```{r}
library(ggplot2)

all_players$plot_name <- gsub("\\s", "\n", all_players$full_name)

ggplot(all_players, aes(weight_pounds, PTS, color = player_game_number, 
                        label = plot_name)) +
  geom_text() +
  theme_minimal()
```

## Why Care?

1. You want to be one of the few people who can read and write the 3 major R dialects. 

2. What are the goals. If you need something rapid, the tidyverse is great. However, it doesn't always play nicely in the broader ecosystem. The beauty of base R is that it doesn't really change, but it is slow. 

3. Speed

Let's see how some basic operations work.

```{r}
library(microbenchmark)

nba_df <- as.data.frame(nba_data)

nba_tibble <- as_tibble(nba_data)

microbenchmark({nba_df[nba_df$PTS > 15, ]}, 
               {filter(nba_tibble, PTS > 15)}, 
               {nba_data[PTS > 15]})
```

Even for a simple operation, that is pretty convincing.

Now with something a little trickier:

```{r}
base_order <- function() {
  nba_df[nba_df$PTS > 15, c("PTS", "PLAYER_NAME")] |> 
    (\(x) x[order(x$PTS), ])()
}

dplyr_order <- function() {
  nba_tibble %>% 
    filter(PTS > 15) %>% 
    select(PTS, PLAYER_NAME) %>% 
    arrange(PTS)
}

dt_order <- function() {
  nba_data[PTS > 15, list(PTS, PLAYER_NAME)][order(PTS)]
}

microbenchmark(base_order(), 
               dplyr_order(), 
               dt_order())
```

# Dates and Times

Another glorious thing that will happen to you is working with dates -- they tend to be a pain.

```{r}
Sys.Date()

format(Sys.Date(), "%m-%d-%Y")
```

And you get things like this:

```{r}
date1 <- "12012018"

date2 <- "20180112"

date3 <- "12-01-2018"

date4 <- "12/01/2018"
```

If you had to merge data based upon these dates, you would be in trouble.

You could, naturally, use some of the string cleaning stuff that we learned to tear them apart and then re-arrange them in a consistent manner.

```{r}
dateStrings <- strsplit(date3, split = "-")

paste(dateStrings[[1]][3], 
      dateStrings[[1]][2], dateStrings[[1]][1], 
      sep = "-")
```

If we wanted to go down the path of that previous chunk, we would need to pass the whole thing into an lapply.

Or...we can just do this:

```{r}
library(lubridate)

mdy(date3)
```

There is some really great stuff in <span class="pack">lubridate</span>. For instance, if you need to get time intervals:

```{r}
exactAge <- function(birthday, 
                     value = c("second", "minute", "hour", "day", 
                               "week", "month", "year")) {
  age = interval(ymd(birthday), ymd(Sys.Date()))  
  
  output <- time_length(age, value)
  
  return(output)
}
```

Run that and give it your birthday in "YEAR-MO-DY" format. While it might be goofy at first glance, can we think of anything practical for it?

There is a similar package, <span class="pack">hms</span>, for dealing with time.